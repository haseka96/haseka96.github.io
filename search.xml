<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DETR_Start</title>
      <link href="/2023/11/01/DETR%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/11/01/DETR%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<center><head>    DETR原理解读    </head> </center><h6 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a>原理解读</h6><hr><p> DETR任务是Object detection，用到的工具是transformers，特点是End-to-end。而目标检测的任务是要预测一系列的Bounding Box的坐标及Label，当下大多数检测器通过定义一些proposal或者anchor，将问题构建成一个分类及回归问题来间接完成这个任务。DETR的工作则是将transfoemers运用到目标检测领域，取代了现在模型需要手工设计的工作，并且取得了不错的结果。    DETR是第一个使用端到端的方法解决检测问题，解决的方法是检测问题视为一个set prediction 问题，如下图所示：</p><p>​    <img src="https://pic1.zhimg.com/v2-772984ccd82a0e0a279ea6a09c3c34c0_r.jpg" alt=""></p><h6 id=""><a href="#" class="headerlink" title=" "></a> </h6><p>     网络的主要组成：CNN和Transformer。</p><p>DETR工作两个关键部分：</p><ul><li>用transformer的encoder-decoder架构一次性生成N个box prediction。(N是一个事先设定的、远远大于image中object个数的一个整数)</li><li>设计了bipartite matching loss，基于预测的box和ground truthboxes的二分图匹配计算loss大小，从而使得预测的box位置和类别更接近于ground truth。</li></ul><p>DETR整体结构可以分为4个部分：(如下图所示)</p><ul><li><p>backbone</p></li><li><p>encoder</p></li><li><p>decoder</p></li><li><p>FFN</p><p><img src="https://pic4.zhimg.com/80/v2-3d43474df51c545ad6bafc19b3c8ccc3_720w.webp" alt=""></p></li></ul><ol><li><p><strong>backbone:</strong> CNN backbone处理 <script type="math/tex">x_i \in B \times 3 \times H_0 \times W_0</script>维的图像，把它转换为$f \in R^{B\times C \times W}$维的feature map。</p></li><li><p><strong>encoder：</strong> encoder的输入是$f \in R^{B\times C \times W}$ 维的feature map，接下来一次进行下面过程：</p><ul><li><strong>通道数压缩:</strong> 通过1*1卷积处理，将channels数量从C压缩到d，即得到$z_0 \in R^{B\times d \times W}$ 维的新feature map。</li><li><strong>转化为序列化数据：</strong> 将空间的维度（高和宽）压缩为一个维度，即把$z_0 \in R^{B\times d \times W}$ 维的新feature map通过reshape成（HW，B，256）维的feature map。</li><li><strong>位置编码：</strong> 在得到$z<em>0 \in R^{B\times d \times W}$ 维的feature map后，正式输入encoder之前，需要进行<strong>Positional Encoding</strong>。因为在<em>_self-attention中需要有表示位置的信息，但是transformer encoder这个结构本身无法体现出位置信息</em></em>。所以我们需要对$z_0 \in R^{B\times d \times W}$ 维的feature map做positional encoding。</li></ul><p>原版本Transformer与Vision Transformer中Positional Encoding表达式为：</p><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(pos / 10000^{2i/d}),  PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d})</script><p>其中，d就是d*HW维的feature map的第一维，$pos \in [1,HW]$。表示token在sequence中的位置，sequence的长度是HW，例如第一个token的pos=0。</p><p>2i和2i+1表示了Positional Encoding的维度，i的取值范围为[0,…,d/2）。所以当pos为1时，Positional Encoding可以写为：</p><script type="math/tex; mode=display">PE(1) = [sin(1/10000^{(0/256)}),cos(1/10000^{(0/256)}),sin(1/10000^{(2/256)}),cos(1/10000^{(2/256)}),...]</script><p>式子中，d=256。</p><p><strong>不同点1：因为Transformer原版中只需要考虑x方向的位置编码，而DETR需要考虑xy方向的位置编码（图像特征是2-D特征），考虑xy方向进行同时编码</strong>Positional Enconding的输出张量：(B,d,H,W),d = 256,其中d代表位置编码长度，H,W代表张量位置。意思为，这个特征图上的任意一个点(H1,W1)有个位置编码，这个编码的长度为256，其中，前128维代表H1的位置编码，后128维代表W1的位置编码。</p><script type="math/tex; mode=display">PE_{(pos_x,2i) = sin(pos_x/10000^{2i/128})}</script><script type="math/tex; mode=display">PE_{(pos_x,2i+1) = cos(pos_x/10000^{2i/128})}</script><script type="math/tex; mode=display">PE_{(pos_y,2i) = sin(pos_y/10000^{2i/128})}</script><script type="math/tex; mode=display">PE_{(pos_y,2i+1) = cos(pos_y/10000^{2i/128})}</script><p>任意一个位置$(pos_x,pos_y),pos_x \in [1,HW],pos_y \in [1,HW]$ 的Positional Encoding，通过公式(3)(4)可以得到128维向量，代表 $pos_x$ 的位置编码，通过带入$pos_y$带入公式(5)(6)可以得到一个128维向量，代表$pos_y$的位置编码，将这两个128维的向量拼接起来，可以得到一个256维的向量，代表$(pos_x,pos_y)$的位置编码。</p></li></ol><p>   计算所有位置的编码后就可以得到(256,H,W)的张量，代表这个batch的位置编码。编码矩阵的维度是(B,256,H,W)，也把其序列化为维度为(HW,B,256)维的张量。准备与<strong>(HW,B,256)维的feature map相加输入Encoder</strong>。</p><p>   <img src="https://pic1.zhimg.com/80/v2-89d23b461169c6ab25ea64389fe8d86c_720w.webp" alt=""></p><p>   <strong>不同点2：原版Transformer只在Encoder之前使用了Positional Encoding，并且只在输入上进行Positional Encoding，再把输入经过transformation matrix变为Query，Key和Value这几个张量。但DETR在Encoder的每一个Multi-head Self-attention之前都使用了Positional Encoding，且只对Query和Key使用，即：只把维度维(HW,B,256)维的位置编码与维度为(HW,B,256)维的Query和Key相加，而不与Value相加</strong>。</p><p>   下图为DETR的transformer的结构详解：</p><p>   <img src="https://pic3.zhimg.com/v2-c158521c7a602382dfa4d85243672df2_r.jpg" alt=""></p><p>   <img src="https://pic4.zhimg.com/v2-1719966a223d98ad48f98c2e4d71add7_r.jpg" alt=""></p><p>   除了Positonal Encoding设置不一样之外，Encoder其他结构一致。每个Encoder Layer包含一个multi-head self-attention的module和一个前馈网络。</p><p>   Encoder最终输出的是(HW,b,256)维的编码矩阵Embedding，并将其输入Decoder。</p><p>   <strong>与原始transformer编码器不同之处：</strong></p><ul><li><p>输入编码器的位置需要考虑2-D空间位置。</p></li><li><p>位置编码向量需要加入每个Encoder Layer中。</p></li><li><p>在编码器内部位置编码仅作用于Query和Key，即只与Query和Key相加，Value不做处理。</p></li></ul><ol><li><p>decoder：</p><p>DETR的Decoder与原版Transformer的也不太一样：原版的decoder最后一个框output probability，代表一次只产生一个单词的softmax，并由此得到这个单词的预测结果。即：<strong>predicts the output sequence one element at a time</strong>。</p></li></ol><p>   不同的是，DETR的transformer decoder是一次性处理全部的object queries，即一次性输出全部的predictions，即：<strong>decodes the N objects in parallel at each decoder layer</strong>。</p><p>   DETR的Decoder主要有两个输入：</p><ol><li>Transformer Encoder 输入的Embedding与position encoding之和。</li><li><p>Object queries。</p><p>其中，Embedding即使上文提到的(HW,b,256)的编码矩阵。</p></li></ol><p>   <strong>Object queries</strong>是一个维度维(100,b,256)的张量，数据类型是nn.Embedding，该张量可学习。<strong>Object queries</strong>矩阵内部通过学习建模了100个物体之间的全局关系（例如：房间里桌子旁一般放椅子），推理时可以利用该全局注意力更好的进行解码预测输出。</p><p>   Decoder的输入初始被初始为维度为(100,b,256)维的全部元素为0的张量，和<strong>Object queries</strong>相加后一起充当<strong>multi-head self-attention的Query和Key。multi-head self-attention的Value为Decoder的输入(全0张量)</strong>。</p><p>   而到了Decoder的multi-head attention时，它的Key和Value来自Encoder的输出张量，维度为(hw,b,256)，    其中Key值还进行位置编码。Query值一部分来自第一个Add and Norm的输出，维度为(100,b,256)的张量，另一部分来自Object queries，充当可学习的位置编码。所以，multi-head attention的Key和Value的维度为(hw,b,256)，而Query的维度为(100,b,256)。</p><p>   每个Decoder的输出维度为(1,b,100,256)，送入后面的前馈网络。</p><p>   故而：Object queries充当的其实是位置编码的作用，只不过它是可学习的位置编码。所以，归纳得：</p><p>   <img src="https://pic1.zhimg.com/v2-6b9de32f5e1174eb3ecfecc2f0335d48_r.jpg" alt=""></p><p>   <strong>损失函数部分解读：</strong></p><p>   Decoder输出维度(b,100,256)的张量，送到2个前馈网络FFN得到class和Bounding Box。他们会得到N=100个预测目标，包括类别和Bounding Box(100远大于图中目标总数)。计算loss时回归分支仅计算物体位置，背景集合忽略。所以DETR输出张量的维度为<strong>分类分支(b,100,class+1)</strong>和<strong>回归分支(b,100,4)</strong>。其中，4是指每个预测目标归一化的$(c_x,c_y,w,h)$。归一化就是出一图片宽高进行归一化。</p><p>   <strong>question: 预测框和真值如何一一对应————-如何知道第47个预测框对应图片里的狗的?</strong></p><p>   DETR: 目标检测任务就是输出无序集合，如何将GT Bounding Box计算loss？</p><p>   一幅图，若第i个物体的真值表达为$y<em>i = (c_i,b_i)$ ，其中，$c_i$表示它的class，$b_i$表示它的Bounding Box。定义$\hat{y} = {\hat{y_i}}^N</em>{i=1}$ 为网络输出的N个预测值。</p><p>   由匈牙利算法，可以找到每个真指对应的预测值：</p><script type="math/tex; mode=display">   \hat{\sigma} = arg \underset{\sigma\in\textstyle \sum_{N}^{}}{min}  \sum_{i}^{N}L_{match}(y_i,\hat{y_{\sigma(i)}}),</script><p>   对于某一个真值$y<em>i$，假设已经找到了这个真值对应的预测值$\hat{y</em>{\sigma(i)}}$，这里$\textstyle \sum<em>{N}^{}$表示所有可能的排列，代表<strong>从真值索引到预测值索引的所有的映射</strong>，然后用$L</em>{match}$最小化$y<em>i$和$\hat{y</em>{\sigma(i)}}$的距离。</p><script type="math/tex; mode=display">   L_{match} = -l_{\{c_i\ne\emptyset \}}\hat{p}_{\sigma(i)}(c_i) + l_{\{c_i\ne\emptyset \}}L_{box}(b_i,\hat{b}_{\sigma{(i)}})</script><p>   意思为：假设当前从真值索引到预测值索引的所有映射为$\sigma$，对于图片中的每个真值ℹ，先找到对应的预测值$\sigma{(i)}$,再看分类网络的结果$\hat{p}<em>{\sigma(i)}(c_i)$,取反作为$L</em>{match}$的第1部分。再计算回归网络的结果$\hat{b}<em>{\sigma{(i)}}$与直值的Bounding Box的差异，即$L</em>{box}(b<em>i,\hat{b}</em>{\sigma{(i)}})$作为$L_{match}$的第2部分。</p><p>   所以，可使$L<em>{match}$最小的排列$\hat{\sigma}$就是我们要找的排列，即：<em>_对于每个真值ℹ来说，$\hat{\sigma}(i)$就是这个真值所对应的预测值的索引</em></em>。</p><p>   接下来，使用上步得到的排列$\hat{\sigma}$，计算匈牙利损失：</p><script type="math/tex; mode=display">   L_{Hungarian}(y,\hat{y}) = \sum_{i=1}^{N}[-log\hat{p}_{\hat{\sigma}(i)}(c_i) + L_{box}(b_i,\hat{b}_{\sigma{(i)}})]</script><p>   其中，$L_{box}$具体为：</p><script type="math/tex; mode=display">   L_{box}(b_i,\hat{b}_{\sigma{(i)}}) = \lambda_{iou}(b_i,\hat{b}_{\sigma(i)}) + \lambda_{L1}\left \| b_i - \hat{b}_{\sigma(i)}) \right \|_1, where    \lambda_{iou},\lambda_{L1} \in R</script><p>   常用的L1 loss对于大小Bounding  Box会有不同的标度，即使它们的相对误差相似。为缓解该问题，这里使用L1 loss和广义loU损耗$L_{iou}$的线性组合，它是比列不变的。</p><p>   <strong>DETR的End-to-End的原理概括</strong></p><ul><li><p><strong>DETR如何训练?</strong></p><p>训练集李的任意一张图片，假设第1张图片，通过模型产生100个预测框Predict Bounding Box，假设这张图片有3个GT Bounding Box，它们分别是Cat,Monkey,Pig。</p><p>​                               <script type="math/tex">(label_{Cat} = 1,label_{Monkey} = 19,label_{Pig} = 80)</script></p><p>问题是：如何知道这100个预测框哪个对应Cat，哪个对应Monkey，哪个对应Pig？</p><p>首先建立一个(100,3)的矩阵，矩阵元素即为公式(7)所得结果。举个例子：比如左上角(1,1)号元素的含义是：第1个预测框对应Cat(label = 1)的情况下的$L<em>{match}$值。我们用<strong>scipy.optimize</strong>这个库中的<strong>linear_sum_assignment</strong>函数找到最优匹配，这个过程称之为：<em>_”匈牙利算法(Hungarian Algorithm)”</em></em>。</p><p>假设<strong>linear_sum_assignment</strong>结果是：第16个预测框对应Cat，第39个预测框对应Monkey，第88个预测框对应Pig。接下来，会将第16、39、88个预测框挑出来安装公式(9)计算这个图片的Loss。最后，将所有的图片都按照这个模式去训练模型。</p></li></ul><ul><li><p><strong>训练完如何用?</strong></p><p>训练完，你的模型学习到了一种能力，即：<strong>模型产生的100个预测框，它指导某个预测框该对应什么Object</strong>，例如，模型学习到：第1个预测框对应Cat(label=1)，第2个预测框对应Dog(label=11)，第3个预测框对应Mouse(label=32)，第4-100个预测框对应……</p></li></ul><ul><li><p><strong>为什么训练完后，模型学习到了一种能力，即：模型产生的100个预测框，它指导某个预测框该对应什么Object？</strong></p><p>前文提到Object queries，它是一个维度为(100,b,256)维的张量，初始化元素维全0。实现方式是<strong>nn.Embedding(num_queries,hidden_dim)</strong>，这里num<em>queries=100，hidden<em>dim=256，它是可以训练的。这里的b指batch size，对于但章图片而言，假设Object queries是一个维度为(100,256)维的张量。在训练完模型后，这个张量也训练完成了，那</em></em>此时的Object queries代表什么呢?</p><p>可以把此时的<strong>Object queries看成100个格子，每个格子都是256维的向量</strong>。训练完成后，这100个格子里<strong>注入了不同Object的位置信息和类别信息</strong>。例如：第一个格子里的这256维的向量代表着Cat这种Object的位置信息，这种信息是通过训练，考虑所有图片的某个位置附近的Cat编码特征，属于和位置有关的全局Car统计信息。</p><p>测试时，倘若图片中有Cat，Monkey，Pig三种物体，该图片会输入到编码器中进行特征编码，假设特征没有丢失，Decoder的<strong>Key</strong>和 <strong>Value</strong>就是编码器输出的编码向量，而<strong>Query</strong>就是Object queries，就是我们的100个格子。</p><p><strong>Query可以看作代表不同Object的信息，而Key和Value可以看作代表图像的全局信息</strong>。</p><p>通过注意力模块，将<strong>Query</strong>和<strong>Key</strong>计算，然后加权<strong>Value</strong>得到解码器输出。对于第1个格子的<strong>Query</strong>会和<strong>Key</strong>中的所有向量进行计算，目的是查找某个位置附近有没有Cat,如果有那么该特征就会加权输出，若没有，输出信息就不会有Cat。</p><p>整个过程计算完成后就可以把编码向量中的Cat，Monkey，Pig的编码嵌入信息提取出来，容纳后后面接上FFN进行分类和回归就比较容易，因为特征已经对齐了。</p><p>总而言之，Object queries在训练过程中，对于N个格子会压缩入对应的位置和类别相关统计信息，在测试阶段就可以利用<strong>Query区和某个图像的编码特征Key，Value</strong>计算，<strong>若图片中刚好有Query想找的特征，比如Cat，则这个特征就能提取出来，最后通过2个FFN进行分类和回归</strong>。Object queries作用非常类似Faster R-CNN中的anchor，这个anchor是可学习的，由于维度比较高，故可以表征的东西丰富，训练时间相应也会越长。</p></li></ul><p><strong>Experiments：</strong></p><p><strong>1.性能对比：</strong></p><p>​       <img src="https://pic4.zhimg.com/80/v2-a82446719e7ebd58ac2b3680bd096b6b_720w.webp" alt=""></p><p><strong>2.编码器层数对比:</strong></p><p><img src="https://pic2.zhimg.com/80/v2-35d68162e148aa1457e7f91d135cfdf1_720w.webp" alt=""></p><p>实验发现，编码器层数越多越好，最后选择6层。</p><p>下图为最后一个Encoder Layer的attention可视化，Encoder已经分离了instances，简化了Decoder的对象提取和定位。</p><p><img src="https://pic3.zhimg.com/80/v2-dffe148c6e78f7b67cf6aa5c8bbbc316_720w.webp" alt=""></p><p><strong>3.解码器层数对比:</strong></p><p><img src="https://pic4.zhimg.com/80/v2-87f7c11d6b088af0d0351e3e4808e4b7_720w.webp" alt=""></p><p>可以发现，性能随着解码器层数的增加而提升。下图为Decoder Layer的attention可视化：</p><p><img src="https://pic1.zhimg.com/80/v2-6b80634bf88e496f035945ec25c40764_720w.webp" alt=""></p><p>类似于可视化编码器注意力，作者用不同颜色给每个预测对象的注意力图着色。</p><p><strong>Final: Question:</strong></p><p><strong>Q:</strong> 如果他的N个object query在训练完之后，每个都有了自己对应的目标，比如cat\monkey\pig等，但是如果测试图中，有好多同一目标该怎么办？这时候的输出(N，class+1)与(N,4)该怎么输出多个同一目标？</p><p><strong>A:</strong> N个object query在训练完之后，每个都有了自己对应的目标”只是一种便于理解的方式，实际上也可能是多个query都能找到同样类型的object。比如第49,65个query都可以对应Cat。但anyway，还是query个数多一点比较好，像Deformable DETR就用了N=300。</p>]]></content>
      
      
      <categories>
          
          <category> -技术 -笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -BEV -DETR </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
