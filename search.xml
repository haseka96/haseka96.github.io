<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DETR代码解读</title>
      <link href="/2023/11/02/DETR%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
      <url>/2023/11/02/DETR%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="detr代码解析">DETR代码解析</h1><h2 id="一源代码">一、源代码</h2><p><code>https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/detr</code></p><h2 id="二代码解析">二、代码解析</h2><h3 id="前言">1.前言</h3><ul><li><p><strong>二维位置编码：</strong></p><p>构造位置矩阵x_embed、y_embed，这里用python函数cumsum，对一个矩阵的元素进行累加，那么累加以后最后一个元素就是对所以累加元素的和，省去了求和的步骤，直接用这个和做归一化，对应x_embed[:,:,-1:]和y_embed[:,-1:,:]。</p></li><li><p><strong>代码中一些变量的shape：</strong></p><p>tensor_list的类型是NestedTensor,内部自动附加mask，用于表示动态shape，是pytorch新特性，全是false。</p><p>x:(b,c,H,W)</p><p>mask: (b,H,W)，全是False。</p><p>not_mask：(b,H,W)，全是True。</p><p>首先出现的y_embed：(b,H,W)，具体是1,1,1,1,......,2,2,2,2,......,3,3,3,3,......</p><p>首先出现的x_embed：(b,H,W)，具体是1,2,3,4,......,1,2,3,4,......,1,2,3,4,......</p><p>self.num_pos_feats = 128</p><p>首先出现的dim_t = [0,1,2,3,......,127]</p><p>pos_x：(b,H,W,128)</p><p>pos_y：(b,H,W,128)</p><p>flatten后面的数字是指：flatten()方法应从哪个轴开始展开操作。</p><p>torch.stack((pos_y[:,:,:,0::2].sin(),pos_y[:,:,:,0::2].cos()),dim=4)</p><p>这一步执行完后变成(b,H,W,2,64)通过flatten()方法从第3个轴开始展平，变为：(b,H,W,128)</p><p>toch.cat((pos_y,pos_x),dim=3)之后变为(b,H,W,256)，最后permute为(b,256,H,W)。</p><p>PositionEmbeddingSine类继承nn.Module类。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> -技术 -笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -BEV -DETR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DETR_Start</title>
      <link href="/2023/11/01/DETR%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/11/01/DETR%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<center><head>DETR原理解读</head></center><h6 id="原理解读">原理解读</h6><hr /><p>DETR任务是Objectdetection，用到的工具是transformers，特点是End-to-end。而目标检测的任务是要预测一系列的BoundingBox的坐标及Label，当下大多数检测器通过定义一些proposal或者anchor，将问题构建成一个分类及回归问题来间接完成这个任务。DETR的工作则是将transfoemers运用到目标检测领域，取代了现在模型需要手工设计的工作，并且取得了不错的结果。DETR是第一个使用端到端的方法解决检测问题，解决的方法是检测问题视为一个setprediction 问题，如下图所示：</p><p>​ <imgsrc="https://pic1.zhimg.com/v2-772984ccd82a0e0a279ea6a09c3c34c0_r.jpg" /></p><h6 id="section"></h6><p>网络的主要组成：CNN和Transformer。</p><p>DETR工作两个关键部分：</p><ul><li>用transformer的encoder-decoder架构一次性生成N个boxprediction。(N是一个事先设定的、远远大于image中object个数的一个整数)</li><li>设计了bipartite matching loss，基于预测的box和groundtruthboxes的二分图匹配计算loss大小，从而使得预测的box位置和类别更接近于groundtruth。</li></ul><p>DETR整体结构可以分为4个部分：(如下图所示)</p><ul><li><p>backbone</p></li><li><p>encoder</p></li><li><p>decoder</p></li><li><p>FFN</p><p><imgsrc="https://pic4.zhimg.com/80/v2-3d43474df51c545ad6bafc19b3c8ccc3_720w.webp" /></p></li></ul><ol type="1"><li><p><strong>backbone:</strong> CNN backbone处理 <spanclass="math display">\[ x_i \in B \times 3 \times H_0 \timesW_0\]</span>维的图像，把它转换为<span class="math inline">\(f \inR^{B\times C \times W}\)</span>维的feature map。</p></li><li><p><strong>encoder：</strong> encoder的输入是<spanclass="math inline">\(f \in R^{B\times C \times W}\)</span> 维的featuremap，接下来一次进行下面过程：</p><ul><li><strong>通道数压缩:</strong>通过1*1卷积处理，将channels数量从C压缩到d，即得到<spanclass="math inline">\(z_0 \in R^{B\times d \times W}\)</span>维的新feature map。</li><li><strong>转化为序列化数据：</strong>将空间的维度（高和宽）压缩为一个维度，即把<spanclass="math inline">\(z_0 \in R^{B\times d \times W}\)</span>维的新feature map通过reshape成（HW，B，256）维的feature map。</li><li><strong>位置编码：</strong> 在得到<span class="math inline">\(z_0\in R^{B\times d \times W}\)</span> 维的featuremap后，正式输入encoder之前，需要进行__PositionalEncoding__。因为在__self-attention中需要有表示位置的信息，但是transformerencoder这个结构本身无法体现出位置信息__。所以我们需要对<spanclass="math inline">\(z_0 \in R^{B\times d \times W}\)</span>维的feature map做positional encoding。</li></ul><p>原版本Transformer与Vision Transformer中Positional Encoding表达式为：$$ PE_{(pos,2i)} = sin(pos / 10000^{2i/d}),</p><p>PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d}) $$其中，d就是d*HW维的feature map的第一维，<span class="math inline">\(pos\in[1,HW]\)</span>。表示token在sequence中的位置，sequence的长度是HW，例如第一个token的pos=0。</p><p>2i和2i+1表示了PositionalEncoding的维度，i的取值范围为[0,...,d/2）。所以当pos为1时，PositionalEncoding可以写为： <span class="math display">\[PE(1) =[sin(1/10000^{(0/256)}),cos(1/10000^{(0/256)}),sin(1/10000^{(2/256)}),cos(1/10000^{(2/256)}),...]\]</span> 式子中，d=256。</p><p>__不同点1：因为Transformer原版中只需要考虑x方向的位置编码，而DETR需要考虑xy方向的位置编码（图像特征是2-D特征），考虑xy方向进行同时编码__PositionalEnconding的输出张量：(B,d,H,W),d =256,其中d代表位置编码长度，H,W代表张量位置。意思为，这个特征图上的任意一个点(H1,W1)有个位置编码，这个编码的长度为256，其中，前128维代表H1的位置编码，后128维代表W1的位置编码。<span class="math display">\[PE_{(pos_x,2i) = sin(pos_x/10000^{2i/128})}\]</span></p><p><span class="math display">\[PE_{(pos_x,2i+1) = cos(pos_x/10000^{2i/128})}\]</span></p><p><span class="math display">\[PE_{(pos_y,2i) = sin(pos_y/10000^{2i/128})}\]</span></p><p><span class="math display">\[PE_{(pos_y,2i+1) = cos(pos_y/10000^{2i/128})}\]</span></p><p>任意一个位置<span class="math inline">\((pos_x,pos_y),pos_x \in[1,HW],pos_y \in [1,HW]\)</span> 的PositionalEncoding，通过公式(3)(4)可以得到128维向量，代表 <spanclass="math inline">\(pos_x\)</span> 的位置编码，通过带入<spanclass="math inline">\(pos_y\)</span>带入公式(5)(6)可以得到一个128维向量，代表<spanclass="math inline">\(pos_y\)</span>的位置编码，将这两个128维的向量拼接起来，可以得到一个256维的向量，代表<spanclass="math inline">\((pos_x,pos_y)\)</span>的位置编码。</p><p>计算所有位置的编码后就可以得到(256,H,W)的张量，代表这个batch的位置编码。编码矩阵的维度是(B,256,H,W)，也把其序列化为维度为(HW,B,256)维的张量。准备与__(HW,B,256)维的featuremap相加输入Encoder__。</p><p><imgsrc="https://pic1.zhimg.com/80/v2-89d23b461169c6ab25ea64389fe8d86c_720w.webp" /></p><p><strong>不同点2：原版Transformer只在Encoder之前使用了PositionalEncoding，并且只在输入上进行PositionalEncoding，再把输入经过transformationmatrix变为Query，Key和Value这几个张量。但DETR在Encoder的每一个Multi-headSelf-attention之前都使用了PositionalEncoding，且只对Query和Key使用，即：只把维度维(HW,B,256)维的位置编码与维度为(HW,B,256)维的Query和Key相加，而不与Value相加</strong>。</p><p>下图为DETR的transformer的结构详解：</p><p><imgsrc="https://pic3.zhimg.com/v2-c158521c7a602382dfa4d85243672df2_r.jpg" /></p><p><imgsrc="https://pic4.zhimg.com/v2-1719966a223d98ad48f98c2e4d71add7_r.jpg" /></p><p>除了PositonalEncoding设置不一样之外，Encoder其他结构一致。每个EncoderLayer包含一个multi-head self-attention的module和一个前馈网络。</p><p>Encoder最终输出的是(HW,b,256)维的编码矩阵Embedding，并将其输入Decoder。</p><p><strong>与原始transformer编码器不同之处：</strong></p><ul><li><p>输入编码器的位置需要考虑2-D空间位置。</p></li><li><p>位置编码向量需要加入每个Encoder Layer中。</p></li><li><p>在编码器内部位置编码仅作用于Query和Key，即只与Query和Key相加，Value不做处理。</p></li></ul></li><li><p>decoder：</p><p>DETR的Decoder与原版Transformer的也不太一样：原版的decoder最后一个框outputprobability，代表一次只产生一个单词的softmax，并由此得到这个单词的预测结果。即：<strong>predictsthe output sequence one element at a time</strong>。</p><p>不同的是，DETR的transformer decoder是一次性处理全部的objectqueries，即一次性输出全部的predictions，即：<strong>decodes the Nobjects in parallel at each decoder layer</strong>。</p><p>DETR的Decoder主要有两个输入：</p><ol type="1"><li>Transformer Encoder 输入的Embedding与position encoding之和。</li><li>Object queries。</li></ol><p>其中，Embedding即使上文提到的(HW,b,256)的编码矩阵。</p><p>__Objectqueries__是一个维度维(100,b,256)的张量，数据类型是nn.Embedding，该张量可学习。__Objectqueries__矩阵内部通过学习建模了100个物体之间的全局关系（例如：房间里桌子旁一般放椅子），推理时可以利用该全局注意力更好的进行解码预测输出。</p><p>Decoder的输入初始被初始为维度为(100,b,256)维的全部元素为0的张量，和__Objectqueries__相加后一起充当__multi-headself-attention的Query和Key。multi-headself-attention的Value为Decoder的输入(全0张量)__。</p><p>而到了Decoder的multi-headattention时，它的Key和Value来自Encoder的输出张量，维度为(hw,b,256)，其中Key值还进行位置编码。Query值一部分来自第一个Add andNorm的输出，维度为(100,b,256)的张量，另一部分来自Objectqueries，充当可学习的位置编码。所以，multi-headattention的Key和Value的维度为(hw,b,256)，而Query的维度为(100,b,256)。</p><p>每个Decoder的输出维度为(1,b,100,256)，送入后面的前馈网络。</p><p>故而：Objectqueries充当的其实是位置编码的作用，只不过它是可学习的位置编码。所以，归纳得：</p><p><imgsrc="https://pic1.zhimg.com/v2-6b9de32f5e1174eb3ecfecc2f0335d48_r.jpg" /></p><p><strong>损失函数部分解读：</strong></p><p>Decoder输出维度(b,100,256)的张量，送到2个前馈网络FFN得到class和BoundingBox。他们会得到N=100个预测目标，包括类别和BoundingBox(100远大于图中目标总数)。计算loss时回归分支仅计算物体位置，背景集合忽略。所以DETR输出张量的维度为__分类分支(b,100,class+1)__和__回归分支(b,100,4)__。其中，4是指每个预测目标归一化的<spanclass="math inline">\((c_x,c_y,w,h)\)</span>。归一化就是出一图片宽高进行归一化。</p><p><strong>question:预测框和真值如何一一对应---------如何知道第47个预测框对应图片里的狗的?</strong></p><p>DETR: 目标检测任务就是输出无序集合，如何将GT BoundingBox计算loss？</p><p>一幅图，若第i个物体的真值表达为<span class="math inline">\(y_i =(c_i,b_i)\)</span> ，其中，<spanclass="math inline">\(c_i\)</span>表示它的class，<spanclass="math inline">\(b_i\)</span>表示它的Bounding Box。定义<spanclass="math inline">\(\hat{y} = \{\hat{y_i}\}^N_{i=1}\)</span>为网络输出的N个预测值。</p><p>由匈牙利算法，可以找到每个真指对应的预测值： <spanclass="math display">\[\hat{\sigma} = arg \underset{\sigma\in\textstyle\sum_{N}^{}}{min}  \sum_{i}^{N}L_{match}(y_i,\hat{y_{\sigma(i)}}),\]</span> 对于某一个真值<spanclass="math inline">\(y_i\)</span>，假设已经找到了这个真值对应的预测值<spanclass="math inline">\(\hat{y_{\sigma(i)}}\)</span>，这里<spanclass="math inline">\(\textstyle\sum_{N}^{}\)</span>表示所有可能的排列，代表__从真值索引到预测值索引的所有的映射__，然后用<spanclass="math inline">\(L_{match}\)</span>最小化<spanclass="math inline">\(y_i\)</span>和<spanclass="math inline">\(\hat{y_{\sigma(i)}}\)</span>的距离。 <spanclass="math display">\[L_{match} = -l_{\{c_i\ne\emptyset \}}\hat{p}_{\sigma(i)}(c_i) +l_{\{c_i\ne\emptyset \}}L_{box}(b_i,\hat{b}_{\sigma{(i)}})\]</span> 意思为：假设当前从真值索引到预测值索引的所有映射为<spanclass="math inline">\(\sigma\)</span>，对于图片中的每个真值ℹ，先找到对应的预测值<spanclass="math inline">\(\sigma{(i)}\)</span>,再看分类网络的结果<spanclass="math inline">\(\hat{p}_{\sigma(i)}(c_i)\)</span>,取反作为<spanclass="math inline">\(L_{match}\)</span>的第1部分。再计算回归网络的结果<spanclass="math inline">\(\hat{b}_{\sigma{(i)}}\)</span>与直值的BoundingBox的差异，即<spanclass="math inline">\(L_{box}(b_i,\hat{b}_{\sigma{(i)}})\)</span>作为<spanclass="math inline">\(L_{match}\)</span>的第2部分。</p><p>所以，可使<spanclass="math inline">\(L_{match}\)</span>最小的排列<spanclass="math inline">\(\hat{\sigma}\)</span>就是我们要找的排列，即：<strong>对于每个真值ℹ来说，<spanclass="math inline">\(\hat{\sigma}(i)\)</span>就是这个真值所对应的预测值的索引</strong>。</p><p>接下来，使用上步得到的排列<spanclass="math inline">\(\hat{\sigma}\)</span>，计算匈牙利损失： <spanclass="math display">\[L_{Hungarian}(y,\hat{y}) =\sum_{i=1}^{N}[-log\hat{p}_{\hat{\sigma}(i)}(c_i) +L_{box}(b_i,\hat{b}_{\sigma{(i)}})]\]</span> 其中，<span class="math inline">\(L_{box}\)</span>具体为：<span class="math display">\[L_{box}(b_i,\hat{b}_{\sigma{(i)}}) =\lambda_{iou}(b_i,\hat{b}_{\sigma(i)}) + \lambda_{L1}\left \| b_i -\hat{b}_{\sigma(i)}) \right \|_1, where    \lambda_{iou},\lambda_{L1}\in R\]</span> 常用的L1 loss对于大小BoundingBox会有不同的标度，即使它们的相对误差相似。为缓解该问题，这里使用L1loss和广义loU损耗<spanclass="math inline">\(L_{iou}\)</span>的线性组合，它是比列不变的。</p><p><strong>DETR的End-to-End的原理概括</strong></p><ul><li><p><strong>DETR如何训练?</strong></p><p>训练集李的任意一张图片，假设第1张图片，通过模型产生100个预测框PredictBounding Box，假设这张图片有3个GT BoundingBox，它们分别是Cat,Monkey,Pig。</p><p>​ <span class="math display">\[(label_{Cat} = 1,label_{Monkey} =19,label_{Pig} = 80)\]</span></p><p>问题是：如何知道这100个预测框哪个对应Cat，哪个对应Monkey，哪个对应Pig？</p><p>首先建立一个(100,3)的矩阵，矩阵元素即为公式(7)所得结果。举个例子：比如左上角(1,1)号元素的含义是：第1个预测框对应Cat(label= 1)的情况下的<spanclass="math inline">\(L_{match}\)</span>值。我们用__scipy.optimize__这个库中的__linear_sum_assignment<strong>函数找到最优匹配，这个过程称之为：</strong>"匈牙利算法(HungarianAlgorithm)"__。</p><p>假设__linear_sum_assignment__结果是：第16个预测框对应Cat，第39个预测框对应Monkey，第88个预测框对应Pig。接下来，会将第16、39、88个预测框挑出来安装公式(9)计算这个图片的Loss。最后，将所有的图片都按照这个模式去训练模型。</p></li><li><p><strong>训练完如何用?</strong></p><p>训练完，你的模型学习到了一种能力，即：<strong>模型产生的100个预测框，它指导某个预测框该对应什么Object</strong>，例如，模型学习到：第1个预测框对应Cat(label=1)，第2个预测框对应Dog(label=11)，第3个预测框对应Mouse(label=32)，第4-100个预测框对应......</p></li><li><p><strong>为什么训练完后，模型学习到了一种能力，即：模型产生的100个预测框，它指导某个预测框该对应什么Object？</strong></p><p>前文提到Objectqueries，它是一个维度为(100,b,256)维的张量，初始化元素维全0。实现方式是__nn.Embedding(num_queries,hidden_dim)__，这里num_queries=100，hidden_dim=256，它是可以训练的。这里的b指batchsize，对于但章图片而言，假设Objectqueries是一个维度为(100,256)维的张量。在训练完模型后，这个张量也训练完成了，那__此时的Objectqueries代表什么呢?</p><p>可以把此时的__Objectqueries看成100个格子，每个格子都是256维的向量__。训练完成后，这100个格子里__注入了不同Object的位置信息和类别信息__。例如：第一个格子里的这256维的向量代表着Cat这种Object的位置信息，这种信息是通过训练，考虑所有图片的某个位置附近的Cat编码特征，属于和位置有关的全局Car统计信息。</p><p>测试时，倘若图片中有Cat，Monkey，Pig三种物体，该图片会输入到编码器中进行特征编码，假设特征没有丢失，Decoder的__Key__和__Value__就是编码器输出的编码向量，而__Query__就是Objectqueries，就是我们的100个格子。</p><p><strong>Query可以看作代表不同Object的信息，而Key和Value可以看作代表图像的全局信息</strong>。</p><p>通过注意力模块，将__Query__和__Key__计算，然后加权__Value__得到解码器输出。对于第1个格子的__Query__会和__Key__中的所有向量进行计算，目的是查找某个位置附近有没有Cat,如果有那么该特征就会加权输出，若没有，输出信息就不会有Cat。</p><p>整个过程计算完成后就可以把编码向量中的Cat，Monkey，Pig的编码嵌入信息提取出来，容纳后后面接上FFN进行分类和回归就比较容易，因为特征已经对齐了。</p><p>总而言之，Objectqueries在训练过程中，对于N个格子会压缩入对应的位置和类别相关统计信息，在测试阶段就可以利用__Query区和某个图像的编码特征Key，Value__计算，<strong>若图片中刚好有Query想找的特征，比如Cat，则这个特征就能提取出来，最后通过2个FFN进行分类和回归</strong>。Objectqueries作用非常类似FasterR-CNN中的anchor，这个anchor是可学习的，由于维度比较高，故可以表征的东西丰富，训练时间相应也会越长。</p></li></ul></li></ol><p><strong>Experiments：</strong></p><p><strong>1.性能对比：</strong></p><p>​ <imgsrc="https://pic4.zhimg.com/80/v2-a82446719e7ebd58ac2b3680bd096b6b_720w.webp" /></p><p><strong>2.编码器层数对比:</strong></p><p><imgsrc="https://pic2.zhimg.com/80/v2-35d68162e148aa1457e7f91d135cfdf1_720w.webp" /></p><p>实验发现，编码器层数越多越好，最后选择6层。</p><p>下图为最后一个EncoderLayer的attention可视化，Encoder已经分离了instances，简化了Decoder的对象提取和定位。</p><p><imgsrc="https://pic3.zhimg.com/80/v2-dffe148c6e78f7b67cf6aa5c8bbbc316_720w.webp" /></p><p><strong>3.解码器层数对比:</strong></p><p><imgsrc="https://pic4.zhimg.com/80/v2-87f7c11d6b088af0d0351e3e4808e4b7_720w.webp" /></p><p>可以发现，性能随着解码器层数的增加而提升。下图为DecoderLayer的attention可视化：</p><p><imgsrc="https://pic1.zhimg.com/80/v2-6b80634bf88e496f035945ec25c40764_720w.webp" /></p><p>类似于可视化编码器注意力，作者用不同颜色给每个预测对象的注意力图着色。</p><p><strong>Final: Question:</strong></p><p><strong>Q:</strong> 如果他的N个objectquery在训练完之后，每个都有了自己对应的目标，比如cat，但是如果测试图中，有好多同一目标该怎么办？这时候的输出(N，class+1)与(N,4)该怎么输出多个同一目标？</p><p><strong>A:</strong> N个objectquery在训练完之后，每个都有了自己对应的目标"只是一种便于理解的方式，实际上也可能是多个query都能找到同样类型的object。比如第49,65个query都可以对应Cat。但anyway，还是query个数多一点比较好，像DeformableDETR就用了N=300。</p>]]></content>
      
      
      <categories>
          
          <category> -技术 -笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -BEV -DETR </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
