<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DETR_Start</title>
      <link href="/2023/11/01/DETR%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/11/01/DETR%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<center><head>    DETR原理解读    </head> </center><h6 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a>原理解读</h6><hr><p> DETR任务是Object detection，用到的工具是transformers，特点是End-to-end。而目标检测的任务是要预测一系列的Bounding Box的坐标及Label，当下大多数检测器通过定义一些proposal或者anchor，将问题构建成一个分类及回归问题来间接完成这个任务。DETR的工作则是将transfoemers运用到目标检测领域，取代了现在模型需要手工设计的工作，并且取得了不错的结果。    DETR是第一个使用端到端的方法解决检测问题，解决的方法是检测问题视为一个set prediction 问题，如下图所示：</p><p>​    <img src="https://pic1.zhimg.com/v2-772984ccd82a0e0a279ea6a09c3c34c0_r.jpg" alt=""></p><h6 id=""><a href="#" class="headerlink" title=" "></a> </h6><p>     网络的主要组成：CNN和Transformer。</p><p>DETR工作两个关键部分：</p><ul><li>用transformer的encoder-decoder架构一次性生成N个box prediction。(N是一个事先设定的、远远大于image中object个数的一个整数)</li><li>设计了bipartite matching loss，基于预测的box和ground truthboxes的二分图匹配计算loss大小，从而使得预测的box位置和类别更接近于ground truth。</li></ul><p>DETR整体结构可以分为4个部分：(如下图所示)</p><ul><li><p>backbone</p></li><li><p>encoder</p></li><li><p>decoder</p></li><li><p>FFN</p><p><img src="https://pic4.zhimg.com/80/v2-3d43474df51c545ad6bafc19b3c8ccc3_720w.webp" alt=""></p></li></ul><ol><li><p><strong>backbone:</strong> CNN backbone处理 <script type="math/tex">x_i \in B \times 3 \times H_0 \times W_0</script>维的图像，把它转换为$f \in R^{B\times C \times W}$维的feature map。</p></li><li><p><strong>encoder：</strong> encoder的输入是$f \in R^{B\times C \times W}$ 维的feature map，接下来一次进行下面过程：</p><ul><li><strong>通道数压缩:</strong> 通过1*1卷积处理，将channels数量从C压缩到d，即得到$z_0 \in R^{B\times d \times W}$ 维的新feature map。</li><li><strong>转化为序列化数据：</strong> 将空间的维度（高和宽）压缩为一个维度，即把$z_0 \in R^{B\times d \times W}$ 维的新feature map通过reshape成（HW，B，256）维的feature map。</li><li><strong>位置编码：</strong> 在得到$z<em>0 \in R^{B\times d \times W}$ 维的feature map后，正式输入encoder之前，需要进行<strong>Positional Encoding</strong>。因为在<em>_self-attention中需要有表示位置的信息，但是transformer encoder这个结构本身无法体现出位置信息</em></em>。所以我们需要对$z_0 \in R^{B\times d \times W}$ 维的feature map做positional encoding。</li></ul><p>原版本Transformer与Vision Transformer中Positional Encoding表达式为：</p><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(pos / 10000^{2i/d}),  PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d})</script><p>其中，d就是d*HW维的feature map的第一维，$pos \in [1,HW]$。表示token在sequence中的位置，sequence的长度是HW，例如第一个token的pos=0。</p><p>2i和2i+1表示了Positional Encoding的维度，i的取值范围为[0,…,d/2）。所以当pos为1时，Positional Encoding可以写为：</p><script type="math/tex; mode=display">PE(1) = [sin(1/10000^{(0/256)}),cos(1/10000^{(0/256)}),sin(1/10000^{(2/256)}),cos(1/10000^{(2/256)}),...]</script><p>式子中，d=256。</p><p><strong>不同点1：因为Transformer原版中只需要考虑x方向的位置编码，而DETR需要考虑xy方向的位置编码（图像特征是2-D特征），考虑xy方向进行同时编码</strong>Positional Enconding的输出张量：(B,d,H,W),d = 256,其中d代表位置编码长度，H,W代表张量位置。意思为，这个特征图上的任意一个点(H1,W1)有个位置编码，这个编码的长度为256，其中，前128维代表H1的位置编码，后128维代表W1的位置编码。</p><script type="math/tex; mode=display">PE_{(pos_x,2i) = sin(pos_x/10000^{2i/128})}</script><script type="math/tex; mode=display">PE_{(pos_x,2i+1) = cos(pos_x/10000^{2i/128})}</script><script type="math/tex; mode=display">PE_{(pos_y,2i) = sin(pos_y/10000^{2i/128})}</script><script type="math/tex; mode=display">PE_{(pos_y,2i+1) = cos(pos_y/10000^{2i/128})}</script><p>任意一个位置$(pos_x,pos_y),pos_x \in [1,HW],pos_y \in [1,HW]$ 的Positional Encoding，通过公式(3)(4)可以得到128维向量，代表 $pos_x$ 的位置编码，通过带入$pos_y$带入公式(5)(6)可以得到一个128维向量，代表$pos_y$的位置编码，将这两个128维的向量拼接起来，可以得到一个256维的向量，代表$(pos_x,pos_y)$的位置编码。</p></li></ol><p>   计算所有位置的编码后就可以得到(256,H,W)的张量，代表这个batch的位置编码。编码矩阵的维度是(B,256,H,W)，也把其序列化为维度为(HW,B,256)维的张量。准备与<strong>(HW,B,256)维的feature map相加输入Encoder</strong>。</p><p>   <img src="https://pic1.zhimg.com/80/v2-89d23b461169c6ab25ea64389fe8d86c_720w.webp" alt=""></p><p>   <strong>不同点2：原版Transformer只在Encoder之前使用了Positional Encoding，并且只在输入上进行Positional Encoding，再把输入经过transformation matrix变为Query，Key和Value这几个张量。但DETR在Encoder的每一个Multi-head Self-attention之前都使用了Positional Encoding，且只对Query和Key使用，即：只把维度维(HW,B,256)维的位置编码与维度为(HW,B,256)维的Query和Key相加，而不与Value相加</strong>。</p><p>   下图为DETR的transformer的结构详解：</p><p>   <img src="https://pic3.zhimg.com/v2-c158521c7a602382dfa4d85243672df2_r.jpg" alt=""></p><p>   <img src="https://pic4.zhimg.com/v2-1719966a223d98ad48f98c2e4d71add7_r.jpg" alt=""></p><p>   除了Positonal Encoding设置不一样之外，Encoder其他结构一致。每个Encoder Layer包含一个multi-head self-attention的module和一个前馈网络。</p><p>   Encoder最终输出的是(HW,b,256)维的编码矩阵Embedding，并将其输入Decoder。</p><p>   <strong>与原始transformer编码器不同之处：</strong></p><ul><li><p>输入编码器的位置需要考虑2-D空间位置。</p></li><li><p>位置编码向量需要加入每个Encoder Layer中。</p></li><li><p>在编码器内部位置编码仅作用于Query和Key，即只与Query和Key相加，Value不做处理。</p></li></ul><ol><li><p>decoder：</p><p>DETR的Decoder与原版Transformer的也不太一样：原版的decoder最后一个框output probability，代表一次只产生一个单词的softmax，并由此得到这个单词的预测结果。即：<strong>predicts the output sequence one element at a time</strong>。</p></li></ol><p>   不同的是，DETR的transformer decoder是一次性处理全部的object queries，即一次性输出全部的predictions，即：<strong>decodes the N objects in parallel at each decoder layer</strong>。</p><p>   DETR的Decoder主要有两个输入：</p><ol><li>Transformer Encoder 输入的Embedding与position encoding之和。</li><li><p>Object queries。</p><p>其中，Embedding即使上文提到的(HW,b,256)的编码矩阵。</p></li></ol><p>   <strong>Object queries</strong>是一个维度维(100,b,256)的张量，数据类型是nn.Embedding，该张量可学习。<strong>Object queries</strong>矩阵内部通过学习建模了100个物体之间的全局关系（例如：房间里桌子旁一般放椅子），推理时可以利用该全局注意力更好的进行解码预测输出。</p><p>   Decoder的输入初始被初始为维度为(100,b,256)维的全部元素为0的张量，和<strong>Object queries</strong>相加后一起充当<strong>multi-head self-attention的Query和Key。multi-head self-attention的Value为Decoder的输入(全0张量)</strong>。</p><p>   而到了Decoder的multi-head attention时，它的Key和Value来自Encoder的输出张量，维度为(hw,b,256)，    其中Key值还进行位置编码。Query值一部分来自第一个Add and Norm的输出，维度为(100,b,256)的张量，另一部分来自Object queries，充当可学习的位置编码。所以，multi-head attention的Key和Value的维度为(hw,b,256)，而Query的维度为(100,b,256)。</p><p>   每个Decoder的输出维度为(1,b,100,256)，送入后面的前馈网络。</p><p>   故而：Object queries充当的其实是位置编码的作用，只不过它是可学习的位置编码。所以，归纳得：</p><p>   <img src="https://pic1.zhimg.com/v2-6b9de32f5e1174eb3ecfecc2f0335d48_r.jpg" alt=""></p><p>   <strong>损失函数部分解读：</strong></p><p>   Decoder输出维度(b,100,256)的张量，送到2个前馈网络FFN得到class和Bounding Box。他们会得到N=100个预测目标，包括类别和Bounding Box(100远大于图中目标总数)。计算loss时回归分支仅计算物体位置，背景集合忽略。所以DETR输出张量的维度为<strong>分类分支(b,100,class+1)</strong>和<strong>回归分支(b,100,4)</strong>。其中，4是指每个预测目标归一化的$(c_x,c_y,w,h)$。归一化就是出一图片宽高进行归一化。</p><p>   <strong>question: 预测框和真值如何一一对应————-如何知道第47个预测框对应图片里的狗的?</strong></p><p>   DETR: 目标检测任务就是输出无序集合，如何将GT Bounding Box计算loss？</p><p>   一幅图，若第i个物体的真值表达为$y<em>i = (c_i,b_i)$ ，其中，$c_i$表示它的class，$b_i$表示它的Bounding Box。定义$\hat{y} = {\hat{y_i}}^N</em>{i=1}$ 为网络输出的N个预测值。</p><ol><li>FFN</li></ol><p><strong>损失函数部分解读：</strong></p><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> -技术 -笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -BEV -DETR </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
